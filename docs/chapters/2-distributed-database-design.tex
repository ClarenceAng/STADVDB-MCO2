%%
%% Section 2: Distributed Database Design
%%
\section{Distributed Database Design}

This section describes the architecture of the distributed database system, including the node configuration, data fragmentation strategy, replication mechanism, and the web application interface.

\subsection{System Architecture}

The system consists of three MySQL database nodes deployed on separate cloud servers. Each node runs independently and communicates with the others through a custom transaction log-based synchronization mechanism. The nodes are configured as follows:

\begin{itemize}
    \item \textbf{Node 1 (Central Node):} Contains all 100,000 records from the dataset. This node serves as the central coordinator and maintains the complete dataset for reference and recovery purposes.
    \item \textbf{Node 2:} Contains records where \texttt{titleType} is one of \texttt{movie}, \texttt{short}, \texttt{video}, or \texttt{videoGame}. This partition holds non-television content.
    \item \textbf{Node 3:} Contains records where \texttt{titleType} is not in the set above, primarily television-related content such as \texttt{tvSeries}, \texttt{tvEpisode}, \texttt{tvMovie}, and \texttt{tvSpecial}.
\end{itemize}

The three nodes are hosted on separate virtual machines (\texttt{stadvdb1}, \texttt{stadvdb2}, \texttt{stadvdb3}) connected to the same local network and synchronize through application-level polling of transaction logs.

\subsection{Database Schema}

The system uses a homogeneous schema across all three nodes. Each node contains the \texttt{DimTitle} table with the following structure:

\begin{verbatim}
CREATE TABLE DimTitle (
    titleID      INT PRIMARY KEY AUTO_INCREMENT,
    tconst       VARCHAR(20),
    titleType    VARCHAR(50),
    primaryTitle VARCHAR(500),
    originalTitle VARCHAR(500),
    isAdult      TINYINT,
    startYear    INT,
    endYear      INT,
    genre1       VARCHAR(50),
    genre2       VARCHAR(50),
    genre3       VARCHAR(50),
    dateCreated  DATETIME,
    dateModified DATETIME,
    version      INT DEFAULT 0
);
\end{verbatim}

The \texttt{version} column supports optimistic concurrency control by tracking the number of updates applied to each record. A trigger increments this value on each update operation.

\subsection{Data Fragmentation Strategy}

The system implements horizontal fragmentation based on the \texttt{titleType} attribute. This fragmentation criterion was selected because:

\begin{enumerate}
    \item It produces a logical separation of content types that aligns with potential use cases (e.g., a service dedicated to movies versus television content).
    \item It creates partitions of comparable size, distributing the workload across nodes.
    \item The \texttt{titleType} value is immutable for existing records, reducing the need for data migration between nodes.
\end{enumerate}

The fragmentation predicates are defined as:
\begin{itemize}
    \item Node 2: \texttt{titleType IN ('movie', 'short', 'video', 'videoGame')}
    \item Node 3: \texttt{titleType NOT IN ('movie', 'short', 'video', 'videoGame')}
\end{itemize}

Nodes 2 and 3 contain non-overlapping subsets. The union of these subsets equals the complete dataset stored in Node 1.

\subsection{Data Replication Strategy}

The system uses a custom transaction log-based replication mechanism rather than relying on MySQL's built-in replication features. This approach provides a finer control over the replication behavior and supports the recovery requirements of the distributed system.

\subsubsection{Transaction Log Tables}

Each node maintains a transaction log table (\texttt{node\{X\}\_transaction\_log}) that records all write operations. The log structure includes:

\begin{itemize}
    \item \texttt{log\_id}: Unique identifier for each log entry
    \item \texttt{operation\_type}: The type of operation (INSERT, UPDATE, DELETE)
    \item \texttt{payload}: JSON representation of the affected record
    \item \texttt{version}: Version number for ordering
    \item \texttt{status}: Current state (pending or committed)
    \item \texttt{origin\_node\_id}: The node where the operation originated
    \item \texttt{created\_at}: Timestamp when the log entry was created
    \item \texttt{committed\_at}: Timestamp when the operation was committed
\end{itemize}

A separate \texttt{latest\_log\_table} tracks the last applied log timestamp from each remote node. This would enable the nodes to determine which logs have not yet been replicated.

\subsubsection{Replication Flow}

The replication process operates as follows:

\begin{enumerate}
    \item When a write operation occurs on any node, it is recorded in that node's transaction log with status \texttt{pending}.
    \item A polling mechanism runs continuously on each node, checking remote nodes for new log entries.
    \item New logs are fetched based on the \texttt{created\_at} timestamp, filtered by the fragmentation predicate, and inserted into the local transaction log.
    \item A separate replication process applies pending logs from remote nodes to the local \texttt{DimTitle} table.
    \item Once applied, the log status is updated to \texttt{committed}.
\end{enumerate}

\subsubsection{Fragmentation-Aware Filtering}

When replicating from Node 1 to Nodes 2 or 3, logs are filtered based on the \texttt{titleType} attribute:

\begin{itemize}
    \item Node 2 accepts only records where \texttt{titleType} does not start with \texttt{tv}.
    \item Node 3 accepts only records where \texttt{titleType} starts with \texttt{tv}.
    \item UPDATE and DELETE operations are replicated to all relevant nodes regardless of \texttt{titleType}.
\end{itemize}

Node 1, as the central node, receives all logs from Nodes 2 and 3 without filtering.

\subsection{Web Application Overview}

The web application consists of two components:

\begin{enumerate}
    \item \textbf{Frontend:} A React-based single-page application built using the CoreUI framework. The interface provides separate views for each node, allowing users to browse, search, insert, and update records.
    \item \textbf{Backend:} An Express.js server that exposes REST API endpoints for database operations. The server maintains connection pools to all three database nodes and routes requests to the appropriate node based on the operation.
\end{enumerate}

The backend server runs on port 4000 and provides endpoints for reading records with pagination (\texttt{/items}), retrieving record counts (\texttt{/count}), and performing write operations. Each request specifies the target node, and the server executes the query against the corresponding database connection.

\subsection{Update and Recovery Mechanism Overview}

This section provides a high-level overview of the update propagation and recovery mechanisms. Detailed algorithms and experimental results are presented in Sections 3 and 4.

\subsubsection{Update Propagation}

The system implements a multi-master architecture where any node can accept write operations. Updates propagate through the following process:

\begin{enumerate}
    \item A client submits a write request (INSERT, UPDATE, or DELETE) to a specific node through the web application.
    \item The node executes the operation locally within a transaction and records the operation in its transaction log.
    \item A background synchronization process runs on each node at regular intervals (every 1 second), performing three steps:
    \begin{itemize}
        \item \textbf{Poll:} Fetch new transaction logs from remote nodes that have timestamps greater than the last applied log.
        \item \textbf{Commit Self:} Mark locally-originated logs as committed.
        \item \textbf{Replicate:} Apply pending logs from remote nodes to the local database.
    \end{itemize}
    \item The replication process uses a \texttt{trigger\_control} table to temporarily disable local triggers during replication, preventing recursive log entries.
\end{enumerate}

\subsubsection{Recovery Mechanism}

The transaction log-based design supports recovery from node failures:

\begin{enumerate}
    \item Each node tracks the last applied log timestamp from every other node in the \texttt{latest\_log\_table}.
    \item When a node becomes unavailable, the polling mechanism detects the connection failure and skips that node for the current cycle.
    \item When the failed node recovers, the polling mechanism resumes and fetches all logs created after the last applied timestamp.
    \item Logs are ordered by version, creation timestamp, and log ID to ensure correct application order.
    \item The recovery process applies missed transactions incrementally, bringing the recovered node back to a consistent state.
\end{enumerate}

This approach ensures that no transactions are lost during node failures, provided the transaction logs remain intact. The system achieves eventual consistency: all nodes converge to the same state once network connectivity is restored and all pending logs are processed.